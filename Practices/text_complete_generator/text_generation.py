# -*- coding: utf-8 -*-
"""text_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9jrGOdd2pGgoNTIIfTj7hpVYv02x9HU
"""

import random
import pickle
import numpy as np
import pandas as pd

from nltk.tokenize import RegexpTokenizer
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Activation
from tensorflow.keras.optimizers import RMSprop

text_df = pd.read_csv("fake_or_real_news.csv")

text_df

text = list(text_df.text.values)
joined_text = " ".join(text)

partial_teext = joined_text[:100000]

#tokenize text
tokenizer = RegexpTokenizer(r"\w+")
tokens = tokenizer.tokenize(partial_teext.lower())

tokens

unique_tokens = np.unique(tokens)
unique_token_index = {token:idx for idx, token in enumerate(unique_tokens)}

unique_token_index

n_words = 10
input_words = []
next_word = []

for i in range(len(tokens) - n_words):
  input_words.append(tokens[i:i + n_words])
  next_word.append(tokens[i+ n_words])

next_word

input_words

#preparing a list tha for ech sample wil looks inside 10 posible words and will mark 0 or 1 according with the binary data value
x = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)
y = np.zeros((len(next_word), len(unique_tokens)), dtype=bool)

x

y

#go through the samples, maped arrays and set the only position from the dictionary to 1 for the naural network
for i, words in enumerate(input_words):
  for j, word in enumerate(words):
    x[i, j, unique_token_index[word]] = 1
  y[i, unique_token_index[next_word[i]]] = 1

model = Sequential()
model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))
model.add(LSTM(128))
model.add(Dense(len(unique_tokens)))
model.add(Activation("softmax"))

model.compile(loss="categorical_crossentropy", optimizer=RMSprop(learning_rate=0.01), metrics=["accuracy"])
model.fit(x, y, batch_size=128, epochs= 30, shuffle = True)

model.save("pretrained_model/mymodel.h5")

model = load_model("pretrained_model/mymodel.h5")

from posixpath import split
def predict_next_word(input_text, n_best):
  input_text = input_text.lower()
  x = np.zeros((1, n_words, len(unique_tokens)))
  for i, word in enumerate(input_text.split()):
      x[0,i, unique_token_index[word]] = 1
  prediction = model.predict(x)[0]
  return np.argpartition(prediction, -n_best)[-n_best:]

possible = predict_next_word("He will run because he", 5)

possible

print([unique_tokens[idx] for idx in possible])

def generate_text(input_text, text_length,creativity=3):
  word_sequence = input_text.split()
  current = 0
  for _ in range(text_length):
    sub_sequence = " ".join(tokenizer.tokenize(" ". join(word_sequence).lower())[current:current + n_words])
    try:
      choice = unique_tokens[random.choice(predict_next_word(sub_sequence, creativity))]
    except:
      choice = random.choice(unique_tokens)

    word_sequence.append(choice)
    current+=1
  return " ".join(word_sequence)

generate_text("I don't know what to do when ", 10)

model = load_model("pretrained_model/mymodel.h5")
posible = predict_next_word("He have to run to his room and he want", 5)

print([unique_tokens[idx] for idx in posible])

generate_text("i want to see what is posible in this world", 10, 20)