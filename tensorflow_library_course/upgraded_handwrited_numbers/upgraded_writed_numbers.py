# -*- coding: utf-8 -*-
"""Upgraded_writed_numbers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fW63qMwYE0jImMTbsV7lIbXoMgn3HQ-F
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import math

#download datasets of mist
datos, metadatos = tfds.load('mnist', as_supervised=True, with_info=True)

#get separated variabloes of traine data (60k) and tests (10k)
datos_entrenamiento,datos_pruebas = datos['train'], datos['test']

#normalize values, passs of 0.255 to 0.1
#(makes the network posible to learn beter and faster)

def normalizar(imagenes, etiquetas):
  imagenes = tf.cast(imagenes, tf.float32)
  imagenes /= 255
  return imagenes, etiquetas

#Normalize training data with the function we have made
datos_entrenamiento = datos_entrenamiento.map(normalizar)
datos_pruebas = datos_pruebas.map(normalizar)

#add cache to use memori instead disk, (faster training)
datos_entrenamiento = datos_entrenamiento.cache()
datos_entrenamiento = datos_pruebas.cache()

clases = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

plt.figure(figsize = (10,10))
for i, (imagen,etiqueta) in enumerate(datos_entrenamiento.take(25)):
  imagen = imagen.numpy().reshape((28,28))
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(imagen,cmap=plt.cm.binary)
  plt.xlabel(clases[etiqueta])

plt.show()

#creation of model (dense model, regular w convulosional networks )

modelo = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), input_shape=(28,28, 1), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2), #2x2 is the size of the array

    tf.keras.layers.Conv2D(64, (3,3), input_shape=(28,28, 1), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2), #2x2 is the size of the array

    tf.keras.layers.Dropout(0.5), #a technique to turn neurons off
    tf.keras.layers.Flatten(), #convert cuadratic image to a simple array
    tf.keras.layers.Dense(units=100, activation='relu'),

    tf.keras.layers.Dense(10, activation = 'softmax')
])

#Compilar el modelo
modelo.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

#Los numeros de datos de entrenamiento y pruebas (60k y 10k)
num_datos_entrenamiento = metadatos.splits["train"].num_examples
num_datos_pruebas = metadatos.splits["test"].num_examples

#Trabajar por lotes
TAMANO_LOTE=32

#Shuffle y repeat hacen que los datos esten mezclados de manera aleatoria
#para que el entrenamiento no se aprenda las cosas en orden
datos_entrenamiento = datos_entrenamiento.repeat().shuffle(num_datos_entrenamiento).batch(TAMANO_LOTE)
datos_pruebas = datos_pruebas.batch(TAMANO_LOTE)

historial = modelo.fit(
    datos_entrenamiento,
    epochs=60,
    steps_per_epoch=math.ceil(num_datos_entrenamiento/TAMANO_LOTE)
)

!mkdir output_folder

modelo.save("upgraded_writed_numbers.h5")

!pip install tensorflowjs

!tensorflowjs_converter --input_format keras upgraded_writed_numbers.h5 output_folder